{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11b5e8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os.path\n",
    "import pathlib\n",
    "import platform \n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2db61bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome Mats , have a wonderful day on your Darwin machine. Your data should be located in /Users/matskerver/Documents/data_tana/TAHMO/location_tahmo\n"
     ]
    }
   ],
   "source": [
    "# For data safety reasons a local path is needed to save the data. Both paths noted here are paths of the \n",
    "# programmers who worked on the Hydrological models. Please replace with your own path.\n",
    "\n",
    "cwd = pathlib.Path().resolve()\n",
    "src = cwd.parent\n",
    "data = src.parent.parent.parent\n",
    "OS_type = platform.system()\n",
    "if OS_type == 'Darwin':\n",
    "    username = 'Mats '\n",
    "    data_path = os.path.join(data, 'data_tana', 'TAHMO', 'location_tahmo')\n",
    "    data_path_location = os.path.join(data, 'data_tana')\n",
    "    data_path_netcdf = os.path.join(data_path_location, 'TAHMO', 'interpolated')\n",
    "    \n",
    "else:\n",
    "    username = 'Mootje'\n",
    "    data_path = os.path.join(data, 'OneDrive - Delft University of Technology', 'TU Delft', 'Master ENVM', 'MDP', 'Model', 'Data', 'TAHMO')\n",
    "    data_path_location = os.path.join(data, 'OneDrive - Delft University of Technology', 'TU Delft', 'Master ENVM', 'MDP', 'Model', 'Data')\n",
    "\n",
    "print(f\"Welcome {username}, have a wonderful day on your {OS_type} machine. Your data should be located in {data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c3e1f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a59ec4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_evaporation_average(ds):\n",
    "    average_evap = ds_trim['evap'].mean().values\n",
    "    return average_evap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6596ce4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The used dataset contained 525 timestamps with potential invalid data.\n",
      "It contained 1666 timestamps where no anomalies were detected.\n",
      "Proceeding to remove these values...\n",
      "Removed 525 values correctly, checking new dataset...\n",
      "No negative values detected, proceeding to average evaporation value\n",
      "Completed. The average evaporation data for this dataset is 0.41639290443606647 mm/day\n"
     ]
    }
   ],
   "source": [
    "# This cell cleans up the Kriging interpolation data as some NaN values were still present in the TAHMO data. \n",
    "# As the evaporation is averaged in time and space it is not needed to have all the datapoints. So this cell\n",
    "# removes any timestep with NaN values and then calculates the average of the timesteps that remain. \n",
    "\n",
    "\n",
    "# Open the kriging NetCDF file and define neccesary variables. Max file size netcdf_file = 4Gb. \n",
    "netcdf_file = 'kriging_results_evap.nc'\n",
    "ds = xr.open_dataset(os.path.join(data_path_netcdf, netcdf_file))\n",
    "\n",
    "variable = 'evap'\n",
    "file_paths = []\n",
    "cleaning = True\n",
    "\n",
    "while cleaning: \n",
    "    \n",
    "    time_indices = range(0, ds.dims['time'], 1)  \n",
    "    yes = 0\n",
    "    no = 0\n",
    "    timestamps = []\n",
    "    \n",
    "    for time_index in time_indices: #We loop through all the timesteps and check them individually\n",
    "        selected_data = ds.isel(time=time_index)\n",
    "        data_array = selected_data[variable]\n",
    "\n",
    "        # Both negative and NaN values indicate potential problems in the data and are detected here\n",
    "        nan_check = data_array.isnull()\n",
    "        negative_check = data_array < 0\n",
    "        has_nan = nan_check.any()\n",
    "        has_negative = negative_check.any()\n",
    "\n",
    "        # If any anomoly is detected it is added to the list of values to be removed\n",
    "        if has_nan.values or has_negative.values:\n",
    "            timestamps.append(time_index)\n",
    "            yes += 1\n",
    "        else: \n",
    "            no += 1\n",
    "    \n",
    "    if (yes == 0):\n",
    "        # When no more potential issues are detected in the file we can proceed to calculate the average value\n",
    "        \n",
    "        cleaning = False\n",
    "        print(f'No negative values detected, proceeding to average evaporation value')\n",
    "        average_evap = calculate_evaporation_average(ds)\n",
    "        print(f'Completed. The average evaporation data for this dataset is {average_evap}mm/day')\n",
    "        break;\n",
    "        \n",
    "        \n",
    "    print(f'The used dataset contained {yes} timestamps with potential invalid data.')\n",
    "    print(f'It contained {no} timestamps where no anomalies were detected.')\n",
    "    print('Proceeding to remove these values...')\n",
    "\n",
    "    #A mask is created containing all the timesteps that don't contain NaN values. They are then selected and \n",
    "    #all the other steps are removed from the dataset. \n",
    "    mask = ~np.isin(range(ds.dims['time']), timestamps)\n",
    "    ds_trim = ds.isel(time=mask)\n",
    "    ds = ds_trim\n",
    "\n",
    "    print(f'Removed {yes} values correctly, checking new dataset...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05d3925",
   "metadata": {},
   "source": [
    "In the test case the value can simply be copied over to the dataset compiling script. In any final application the file can simply be saved as a NetCDF or .csv file. Alternatively, the function can be simply put in a separate script and then imported in another notebook. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
