{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import glob\n",
    "import platform\n",
    "import pathlib  \n",
    "import os.path\n",
    "import netCDF4 as nc\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.errors import RasterioIOError\n",
    "from shapely.geometry import Point\n",
    "import time\n",
    "import json\n",
    "from scipy.spatial import cKDTree\n",
    "from concurrent.futures import ProcessPoolExecutor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = pathlib.Path().resolve()\n",
    "src = cwd.parent\n",
    "data = src.parent.parent.parent\n",
    "OS_type = platform.system()\n",
    "\n",
    "if OS_type == 'Darwin':\n",
    "    username = 'Mats '\n",
    "    data_path = os.path.join(data, 'data_tana', 'catchments')\n",
    "    shape_path = os.path.join(data, 'data_tana', 'catchments')\n",
    "    results_path = os.path.join(data, 'data_tana', 'catchments', 'results')\n",
    "    evaporation = os.path.join(data, 'data_tana', 'TAHMO', 'interpolated')\n",
    "    \n",
    "else:\n",
    "    username = 'Mootje'\n",
    "    data_path = os.path.join(data, 'OneDrive - Delft University of Technology', 'TU Delft', 'Master ENVM', 'MDP', 'Model', 'Data', 'Satellite')\n",
    "    shape_path = os.path.join(data, 'OneDrive - Delft University of Technology', 'TU Delft', 'Master ENVM', 'MDP', 'Model', 'Data', 'Shapefiles','Mini_shapes')\n",
    "\n",
    "print(f\"Welcome {username}, have a wondeful day on your {OS_type} machine. Your data should be located in {data_path}\")\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_path)\n",
    "\n",
    "# path_subcatchments = os.path.join(data, 'OneDrive - Delft University of Technology', 'TU Delft', 'Master ENVM', 'MDP', 'Model', 'Data','Shapefiles', 'Mini_shapes')\n",
    "# path_satellite = os.path.join(data, 'OneDrive - Delft University of Technology', 'TU Delft', 'Master ENVM', 'MDP', 'Model', 'Data', 'Satellite')\n",
    "# path_save_new_shapefiles = os.path.join(data, 'OneDrive - Delft University of Technology', 'TU Delft', 'Master ENVM', 'MDP', 'Model', 'Data', 'subcatchments_chirps')\n",
    "\n",
    "# shapefiles_fnames = os.path.join(path_subcatchments, '*.gpkg')\n",
    "# shape_files = glob.glob(shapefiles_fnames)\n",
    "# path_satellite = path_satellite.replace('\\\\', '\\\\\\\\')\n",
    "\n",
    "# print(path_satellite)\n",
    "\n",
    "# N = len(shapefiles_fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = glob.glob(os.path.join(data_path, '*.nc'))\n",
    "data_path_evap = os.path.join(evaporation, 'kriging_results_evap.nc')\n",
    "print(data_files)\n",
    "datasets = {}\n",
    "\n",
    "for file_path in data_files:\n",
    "    # Extract the file identifier from the file name\n",
    "    file_name = os.path.basename(file_path)  # Get just the file name\n",
    "    file_identifier = file_name.split('_')[0]  # Split by underscore and take the first part\n",
    "    # print(file_identifier)\n",
    "    # Open the dataset\n",
    "    dataset = xr.open_dataset(file_path)\n",
    "    dataset_evap = xr.open_dataset(data_path_evap)\n",
    "    # print(dataset)\n",
    "    # Add the file identifier as a new coordinate\n",
    "    dataset = dataset.assign_coords(file_identifier=file_identifier)\n",
    "    #dataset_evap = dataset.assign_coords(file_identifier=file_identifier)\n",
    "    \n",
    "    # Add the dataset to the dictionary with the file identifier as the key\n",
    "    datasets[file_identifier] = dataset\n",
    "    # print(datasets[file_identifier])\n",
    "chirps_file = data_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shape_file = os.path.join(shape_path, '*.gpkg')\n",
    "shape_files = glob.glob(shape_file)\n",
    "print(shape_file)\n",
    "gdfs = []\n",
    "\n",
    "for file in shape_files:\n",
    "    gdf = gpd.read_file(file)\n",
    "    gdfs.append(gdf)\n",
    "\n",
    "merged_gdfs = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True))\n",
    "merged_gdfs.plot(color = 'bisque', edgecolor = 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute = False\n",
    "\n",
    "if (execute == True):\n",
    "    def clip_netCDF_to_geopackage(netCDF_file, geopackage_file):\n",
    "        # Open the GeoPackage file\n",
    "        geopackage_gdf = gpd.read_file(geopackage_file)\n",
    "\n",
    "        # Open the NetCDF file\n",
    "        with rasterio.open(netCDF_file) as src:\n",
    "            # Read the NetCDF data\n",
    "            data = src.read(1)\n",
    "\n",
    "            # Clip the NetCDF data to the GeoPackage boundaries\n",
    "            clipped_data, _ = mask(src, geopackage_gdf.geometry, crop=True, nodata=np.nan)\n",
    "\n",
    "            # Get metadata for the clipped data\n",
    "            meta = src.meta.copy()\n",
    "\n",
    "        return clipped_data, meta\n",
    "\n",
    "    # Example usage:\n",
    "    i = 0\n",
    "    start_time = time.time()\n",
    "    clipped_data = []\n",
    "\n",
    "    netCDF_file = data_files[0]  # Assuming data_files contains the list of NetCDF file paths\n",
    "    for geopackage_file in shape_files:  # Assuming shape_files is a list of GeoPackage file paths\n",
    "        clipped_data_current, meta = clip_netCDF_to_geopackage(netCDF_file, geopackage_file)\n",
    "        clipped_data.append(clipped_data_current)\n",
    "        print(geopackage_file)\n",
    "        i += 1\n",
    "        if i > 10:\n",
    "            break;\n",
    "        print(clipped_data_current.shape)\n",
    "    # clipped_data, meta = clip_netCDF_to_geopackage(chirps_file, shape_files)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate and print the execution time\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to add the precipitation and location data to the clipped shapefiles\n",
    "execute = True\n",
    "\n",
    "\n",
    "if (execute == True):\n",
    "    def clip_netCDF_to_geopackage(netCDF_file, geopackage_file):\n",
    "        # Obtain the name of the shapefile from the GeoPackage file\n",
    "        shapefile_name = os.path.basename(geopackage_file)\n",
    "\n",
    "        # Open the GeoPackage file\n",
    "        geopackage_gdf = gpd.read_file(geopackage_file)\n",
    "        geopackage_gdf = geopackage_gdf.to_crs('EPSG:4326')\n",
    "        \n",
    "\n",
    "        #geopackage_gdf = geopackage_gdf.to_crs('EPSG:32737')\n",
    "        # Open the NetCDF file\n",
    "        with rasterio.open(netCDF_file) as src:\n",
    "            # Read the entire timeseries data\n",
    "            data = src.read()\n",
    "\n",
    "            # Clip the NetCDF data to the GeoPackage boundaries\n",
    "            clipped_data, transform = mask(src, geopackage_gdf.geometry, crop=True, nodata=np.nan)\n",
    "            #clipped_data_evap, transform_evap = mask(src_evap, geopackage_gdf_evap.geometry, crop=True, nodata=np.nan)\n",
    "            \n",
    "            # Calculate the area of each clipped geometry\n",
    "            averaged_clipped_data = np.nanmean(clipped_data, axis=(1, 2)) \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            clipped_areas = geopackage_gdf.geometry.area\n",
    "            \n",
    "            # Create an xarray Dataset from the clipped data\n",
    "            '''clipped_dataset = xr.DataArray(\n",
    "                clipped_data,\n",
    "                dims=[\"time\", \"y\", \"x\"],\n",
    "                coords={\"time\": np.arange(clipped_data.shape[0]), \"y\": np.arange(clipped_data.shape[1]), \"x\": np.arange(clipped_data.shape[2])}\n",
    "            ).to_dataset(name=\"precipitation\")'''\n",
    "            \n",
    "            clipped_dataset = xr.DataArray(\n",
    "                averaged_clipped_data,\n",
    "                dims=[\"time\"],\n",
    "                coords={\"time\": np.arange(averaged_clipped_data.shape[0])}\n",
    "            ).to_dataset(name=\"precipitation\")\n",
    "\n",
    "                # Add the clipped areas as a new coordinate variable to the dataset\n",
    "            clipped_dataset.coords['area_m2'] = (('gdf'), clipped_areas)\n",
    "            \n",
    "            average_evap_da = xr.DataArray([average_evap] * clipped_dataset.dims['time'], dims=[\"time\"], coords={\"time\": clipped_dataset.coords[\"time\"]})\n",
    "\n",
    "            # Add this DataArray to your existing dataset as a new variable\n",
    "            clipped_dataset['average_evap'] = average_evap_da\n",
    "\n",
    "        return {shapefile_name: clipped_dataset}\n",
    "\n",
    "\n",
    "\n",
    "    # Assuming data_files and shape_files are already defined\n",
    "    dataset_list = []\n",
    "    i = 0\n",
    "    average_evap = 0.41639290443606647\n",
    "\n",
    "    # Remove the break condition to process all shapefiles\n",
    "    netCDF_file = data_files[0]\n",
    "    start_time = time.time()\n",
    "    for geopackage_file in shape_files:\n",
    "        clipped_dataset = clip_netCDF_to_geopackage(netCDF_file, geopackage_file)\n",
    "        #clipped_dataset = clip_netCDF_to_geopackage(netCDF_file_evap, geopackage_file, area = False)\n",
    "        dataset_list.append(clipped_dataset)\n",
    "        i  += 1\n",
    "        if i > 2:\n",
    "            break\n",
    "        print('Processing:', geopackage_file)\n",
    "    \n",
    "\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution time: {execution_time} seconds\")\n",
    "    # Combine all datasets into one (if necessary and feasible)\n",
    "    # combined_dataset = xr.concat(dataset_list, dim='gdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_list[1])\n",
    "has_non_nan = dataset_list[0]['fid_413.gpkg']['precipitation'].notnull().any()\n",
    "\n",
    "print(\"Are there any non-NaN values in the precipitation array?\", has_non_nan.values)\n",
    "#clipped_dataset['fid_413.gpkg']['precipitation']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "'''for dataset_dict in dataset_list:\n",
    "    for key, ds in dataset_dict.items():\n",
    "        filename = f\"{key.replace('.gpkg', '')}.nc\"  # Creating a filename for each dataset\n",
    "        file_location = os.path.join(data_path, filename)\n",
    "        ds.to_netcdf(file_location)\n",
    "        #print(f\"Saved {filename}\")'''\n",
    "\n",
    "\n",
    "geopackage_gdf = gpd.read_file(geopackage_file)\n",
    "print(geopackage_gdf.crs)\n",
    "geopackage_gdf = geopackage_gdf.to_crs('EPSG:4326')\n",
    "print(geopackage_gdf.crs)\n",
    "\n",
    "print('shapes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def clip_netCDF_to_geopackage(netCDF_file, geopackage_file, netCDF_file_evap):\n",
    "    # Obtain the name of the shapefile from the GeoPackage file\n",
    "    shapefile_name = os.path.basename(geopackage_file)\n",
    "\n",
    "    # Open the GeoPackage file\n",
    "    geopackage_gdf = gpd.read_file(geopackage_file)\n",
    "\n",
    "    # Open the NetCDF file for precipitation\n",
    "    with rasterio.open(netCDF_file) as src:\n",
    "        # Read the entire timeseries data for precipitation\n",
    "        data = src.read()\n",
    "        # Clip the NetCDF data to the GeoPackage boundaries\n",
    "        clipped_data, transform = mask(src, geopackage_gdf.geometry, crop=True, nodata=np.nan)\n",
    "\n",
    "    # Open the NetCDF file for evaporation\n",
    "    with rasterio.open(netCDF_file_evap) as src_evap:\n",
    "        # Read the entire timeseries data for evaporation\n",
    "        data_evap = src_evap.read()\n",
    "        # Clip the NetCDF data to the GeoPackage boundaries\n",
    "        clipped_data_evap, transform_evap = mask(src_evap, geopackage_gdf_evap.geometry, crop=True, nodata=np.nan)\n",
    "\n",
    "    # Create xarray DataArrays from the clipped data\n",
    "    precipitation_da = xr.DataArray(\n",
    "        clipped_data,\n",
    "        dims=[\"time\", \"y\", \"x\"],\n",
    "        coords={\"time\": np.arange(clipped_data.shape[0]), \"y\": np.arange(clipped_data.shape[1]), \"x\": np.arange(clipped_data.shape[2])},\n",
    "        name=\"precipitation\"\n",
    "    )\n",
    "\n",
    "    evaporation_da = xr.DataArray(\n",
    "        clipped_data_evap,\n",
    "        dims=[\"time\", \"y\", \"x\"],\n",
    "        coords={\"time\": np.arange(clipped_data_evap.shape[0]), \"y\": np.arange(clipped_data_evap.shape[1]), \"x\": np.arange(clipped_data_evap.shape[2])},\n",
    "        name=\"evaporation\"\n",
    "    )\n",
    "\n",
    "    # Combine the DataArrays into a single Dataset\n",
    "    clipped_dataset = xr.merge([precipitation_da, evaporation_da])\n",
    "\n",
    "    return {shapefile_name: clipped_dataset}\n",
    "\n",
    "dataset_list = []\n",
    "i = 0\n",
    "\n",
    "# Remove the break condition to process all shapefiles\n",
    "netCDF_file = data_files[0]\n",
    "start_time = time.time()\n",
    "for geopackage_file in shape_files:\n",
    "    clipped_dataset = clip_netCDF_to_geopackage(netCDF_file, geopackage_file, dataset_evap)\n",
    "    #clipped_dataset = clip_netCDF_to_geopackage(netCDF_file_evap, geopackage_file, area = False)\n",
    "    dataset_list.append(clipped_dataset)\n",
    "    i  += 1\n",
    "    if i > 1:\n",
    "        break\n",
    "    print('Processing:', geopackage_file)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "'''\n",
    "x=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapefile_name = os.path.splitext(os.path.basename(geopackage_file))[0]\n",
    "clipped_dataset['fid_1027.gpkg']['precipitation']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shapefile_names(folder_path):\n",
    "    shapefile_names = [file for file in os.listdir(folder_path) if file.endswith('.gpkg')]\n",
    "    return shapefile_names\n",
    "shapefile_name = get_shapefile_names(shape_path)\n",
    "shapefile_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_shape_path = os.path.join(data, 'OneDrive - Delft University of Technology', 'TU Delft', 'Master ENVM', 'MDP', 'Model', 'Data', 'Shapefiles', 'Mini_shapes')\n",
    "mini_shape_file = os.path.join(mini_shape_path, '*.gpkg')\n",
    "mini_shape_files = glob.glob(mini_shape_file)\n",
    "gdfs_mini = []\n",
    "\n",
    "print(mini_shape_files)\n",
    "\n",
    "for file in mini_shape_files:\n",
    "    gdf_mini = gpd.read_file(file)\n",
    "    gdfs_mini.append(gdf_mini)\n",
    "\n",
    "merged_gdfs_mini = gpd.GeoDataFrame(pd.concat(gdfs_mini, ignore_index=True))\n",
    "merged_gdfs_mini.plot(color = 'bisque', edgecolor = 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select one day from the chirps file\n",
    "\n",
    "mini_1_file = mini_shape_files[100]\n",
    "mini_1_shape = gpd.read_file(mini_1_file)\n",
    "mini_1_shape.plot(color= 'bisque')\n",
    "ds = xr.open_dataset(data_files[0])\n",
    "ds_day_1 = ds.sel(time = '2018-01-01') # --> Selected day\n",
    "# print(mini_1_shape)\n",
    "shape_1 = gpd.read_file(mini_shape_files[0])\n",
    "\n",
    "clip_1 = shape_1.geometry.values[0]\n",
    "\n",
    "# print(clip_1)\n",
    "\n",
    "\n",
    "# with rasterio.open(data_files[0]) as src:\n",
    "#     # Clip the dataset to the clipping geometry\n",
    "#     clipped_mini_data, clipped_transform = mask(src, [clip_1], crop=True)\n",
    "\n",
    "# print(clipped_mini_data)\n",
    "\n",
    "# geopackage_gdf = gpd.read_file(mini_1_shape)\n",
    "# clipped_mini, _ = mask(src, geopackage_gdf.geometry.values[0], crop = True, nodata=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(data_files[0])  # Assuming data_files contains the list of NetCDF file paths\n",
    "ds_day_1 = ds.sel(time='2018-01-01')  # Selected day\n",
    "\n",
    "# Load the GeoPackage shapefile\n",
    "shape_1 = gpd.read_file(mini_shape_files[0])  # Assuming mini_shape_files contains the path to the shapefile\n",
    "clip_1 = shape_1.geometry.values[0]  # Assuming the shapefile has only one geometry\n",
    "\n",
    "# Open the CHIRPS file with rasterio\n",
    "with rasterio.open(data_files[0]) as src:\n",
    "    # Clip the CHIRPS data to the clipping geometry\n",
    "    clipped_mini, _ = mask(src, [clip_1], crop=True, nodata=np.nan)\n",
    "\n",
    "plt.imshow(clipped_mini, cmap='jet')\n",
    "plt.colorbar(label='Precipitation (mm)')\n",
    "plt.title('Clipped CHIRPS Data')\n",
    "plt.xlabel('Column')\n",
    "plt.ylabel('Row')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
